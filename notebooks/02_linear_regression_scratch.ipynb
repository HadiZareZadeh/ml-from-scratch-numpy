{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression From Scratch\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "Linear regression models the relationship between features **X** and target **y** as:\n",
        "\n",
        "$$\\hat{y} = \\mathbf{X}\\mathbf{w} + b$$\n",
        "\n",
        "where:\n",
        "- $\\mathbf{w}$ is the weight vector (learned parameters)\n",
        "- $b$ is the bias term\n",
        "- $\\hat{y}$ are the predictions\n",
        "\n",
        "### Loss Function: Mean Squared Error\n",
        "\n",
        "$$L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n}||\\mathbf{y} - \\hat{\\mathbf{y}}||^2$$\n",
        "\n",
        "With L2 regularization (Ridge regression):\n",
        "\n",
        "$$L = \\frac{1}{n}||\\mathbf{y} - \\hat{\\mathbf{y}}||^2 + \\frac{\\lambda}{2}||\\mathbf{w}||^2$$\n",
        "\n",
        "### Gradient Descent\n",
        "\n",
        "We minimize the loss by iteratively updating parameters:\n",
        "\n",
        "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{w}}$$\n",
        "\n",
        "$$b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
        "\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "### Gradients\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{w}} = -\\frac{2}{n}\\mathbf{X}^T(\\mathbf{y} - \\hat{\\mathbf{y}}) + \\lambda\\mathbf{w}$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = -\\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from linear_regression import LinearRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed data\n",
        "with open('../data_preprocessed.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train']\n",
        "X_test = data['X_test']\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Gradient descent converges faster when features are on similar scales. We'll standardize features to have zero mean and unit variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature statistics after scaling:\")\n",
        "print(f\"Mean: {X_train_scaled.mean(axis=0)}\")\n",
        "print(f\"Std: {X_train_scaled.std(axis=0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Linear Regression Model\n",
        "\n",
        "Let's train our from-scratch implementation with different hyperparameters to observe convergence behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model without regularization\n",
        "model_no_reg = LinearRegression(\n",
        "    learning_rate=0.01,\n",
        "    max_iterations=2000,\n",
        "    regularization=0.0,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "model_no_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nFinal training loss: {model_no_reg.loss_history[-1]:.6f}\")\n",
        "print(f\"Number of iterations: {len(model_no_reg.loss_history)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model with L2 regularization\n",
        "model_with_reg = LinearRegression(\n",
        "    learning_rate=0.01,\n",
        "    max_iterations=2000,\n",
        "    regularization=0.1,  # L2 regularization strength\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "model_with_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nFinal training loss: {model_with_reg.loss_history[-1]:.6f}\")\n",
        "print(f\"Number of iterations: {len(model_with_reg.loss_history)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Training Convergence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot loss convergence\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Full convergence\n",
        "axes[0].plot(model_no_reg.loss_history, label='No Regularization', linewidth=2)\n",
        "axes[0].plot(model_with_reg.loss_history, label='L2 Regularization (λ=0.1)', linewidth=2)\n",
        "axes[0].set_xlabel('Iteration')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss Convergence')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# First 200 iterations (zoomed in)\n",
        "axes[1].plot(model_no_reg.loss_history[:200], label='No Regularization', linewidth=2)\n",
        "axes[1].plot(model_with_reg.loss_history[:200], label='L2 Regularization (λ=0.1)', linewidth=2)\n",
        "axes[1].set_xlabel('Iteration')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Training Loss Convergence (First 200 Iterations)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred_no_reg = model_no_reg.predict(X_test_scaled)\n",
        "y_pred_with_reg = model_with_reg.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "mse_no_reg = mean_squared_error(y_test, y_pred_no_reg)\n",
        "mse_with_reg = mean_squared_error(y_test, y_pred_with_reg)\n",
        "\n",
        "r2_no_reg = model_no_reg.score(X_test_scaled, y_test)\n",
        "r2_with_reg = model_with_reg.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"No Regularization:\")\n",
        "print(f\"  MSE: {mse_no_reg:.6f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mse_no_reg):.6f}\")\n",
        "print(f\"  R² Score: {r2_no_reg:.6f}\")\n",
        "print(f\"\\nWith L2 Regularization (λ=0.1):\")\n",
        "print(f\"  MSE: {mse_with_reg:.6f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mse_with_reg):.6f}\")\n",
        "print(f\"  R² Score: {r2_with_reg:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual values\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# No regularization\n",
        "axes[0].scatter(y_test, y_pred_no_reg, alpha=0.5, s=20)\n",
        "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
        "             'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Actual Values')\n",
        "axes[0].set_ylabel('Predicted Values')\n",
        "axes[0].set_title(f'Predictions vs Actual (No Regularization)\\nR² = {r2_no_reg:.4f}')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# With regularization\n",
        "axes[1].scatter(y_test, y_pred_with_reg, alpha=0.5, s=20)\n",
        "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
        "             'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[1].set_xlabel('Actual Values')\n",
        "axes[1].set_ylabel('Predicted Values')\n",
        "axes[1].set_title(f'Predictions vs Actual (L2 Regularization)\\nR² = {r2_with_reg:.4f}')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Rate Experiment\n",
        "\n",
        "Let's observe how different learning rates affect convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different learning rates\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
        "models_lr = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = LinearRegression(\n",
        "        learning_rate=lr,\n",
        "        max_iterations=1000,\n",
        "        regularization=0.0,\n",
        "        verbose=False\n",
        "    )\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    models_lr[lr] = model\n",
        "    print(f\"LR={lr}: Final loss = {model.loss_history[-1]:.6f}, Iterations = {len(model.loss_history)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot convergence for different learning rates\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for lr in learning_rates:\n",
        "    plt.plot(models_lr[lr].loss_history, label=f'LR = {lr}', linewidth=2)\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Effect of Learning Rate on Convergence')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')  # Log scale for better visualization\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Observations\n",
        "\n",
        "1. **Convergence**: Gradient descent successfully minimizes the loss function\n",
        "2. **Regularization**: L2 regularization can help prevent overfitting (though effect may be subtle on this dataset)\n",
        "3. **Learning Rate**: Too small → slow convergence, too large → instability/divergence\n",
        "4. **Vectorization**: NumPy operations enable efficient computation on large datasets\n",
        "\n",
        "The implementation demonstrates core ML principles: optimization, regularization, and hyperparameter sensitivity.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
