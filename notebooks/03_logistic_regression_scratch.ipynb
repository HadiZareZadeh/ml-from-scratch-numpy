{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression From Scratch\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "Logistic regression extends linear regression to binary classification by applying the sigmoid activation function:\n",
        "\n",
        "$$p = \\sigma(\\mathbf{X}\\mathbf{w} + b) = \\frac{1}{1 + e^{-(\\mathbf{X}\\mathbf{w} + b)}}$$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function, mapping logits to probabilities in [0, 1].\n",
        "\n",
        "### Loss Function: Binary Cross-Entropy\n",
        "\n",
        "$$L = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)]$$\n",
        "\n",
        "With L2 regularization:\n",
        "\n",
        "$$L = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)] + \\frac{\\lambda}{2}||\\mathbf{w}||^2$$\n",
        "\n",
        "### Gradients\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{1}{n}\\mathbf{X}^T(\\mathbf{p} - \\mathbf{y}) + \\lambda\\mathbf{w}$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n}(p_i - y_i)$$\n",
        "\n",
        "### Decision Rule\n",
        "\n",
        "$$\\hat{y} = \\begin{cases} 1 & \\text{if } p \\geq 0.5 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from logistic_regression import LogisticRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting Regression to Classification\n",
        "\n",
        "For demonstration purposes, we'll convert the California Housing regression problem into a binary classification task by thresholding the median house value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X = housing.data.values\n",
        "y = housing.target.values\n",
        "\n",
        "# Convert to binary classification: 1 if median house value > median, else 0\n",
        "threshold = np.median(y)\n",
        "y_binary = (y > threshold).astype(int)\n",
        "\n",
        "print(f\"Threshold: {threshold:.2f}\")\n",
        "print(f\"Class distribution:\")\n",
        "print(f\"  Class 0 (≤ {threshold:.2f}): {np.sum(y_binary == 0)} samples\")\n",
        "print(f\"  Class 1 (>{threshold:.2f}): {np.sum(y_binary == 1)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set: {X_train_scaled.shape}\")\n",
        "print(f\"Test set: {X_test_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Logistic Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model without regularization\n",
        "model_no_reg = LogisticRegression(\n",
        "    learning_rate=0.1,\n",
        "    max_iterations=2000,\n",
        "    regularization=0.0,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "model_no_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nFinal training loss: {model_no_reg.loss_history[-1]:.6f}\")\n",
        "print(f\"Number of iterations: {len(model_no_reg.loss_history)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model with L2 regularization\n",
        "model_with_reg = LogisticRegression(\n",
        "    learning_rate=0.1,\n",
        "    max_iterations=2000,\n",
        "    regularization=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "model_with_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nFinal training loss: {model_with_reg.loss_history[-1]:.6f}\")\n",
        "print(f\"Number of iterations: {len(model_with_reg.loss_history)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot loss convergence\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(model_no_reg.loss_history, label='No Regularization', linewidth=2)\n",
        "plt.plot(model_with_reg.loss_history, label='L2 Regularization (λ=0.1)', linewidth=2)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss (Binary Cross-Entropy)')\n",
        "plt.title('Training Loss Convergence')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred_no_reg = model_no_reg.predict(X_test_scaled)\n",
        "y_pred_with_reg = model_with_reg.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "acc_no_reg = model_no_reg.score(X_test_scaled, y_test)\n",
        "acc_with_reg = model_with_reg.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"No Regularization:\")\n",
        "print(f\"  Accuracy: {acc_no_reg:.4f}\")\n",
        "print(f\"\\nWith L2 Regularization (λ=0.1):\")\n",
        "print(f\"  Accuracy: {acc_with_reg:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "cm_no_reg = confusion_matrix(y_test, y_pred_no_reg)\n",
        "cm_with_reg = confusion_matrix(y_test, y_pred_with_reg)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(cm_no_reg, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title('Confusion Matrix (No Regularization)')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "sns.heatmap(cm_with_reg, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
        "axes[1].set_title('Confusion Matrix (L2 Regularization)')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification reports\n",
        "print(\"Classification Report - No Regularization:\")\n",
        "print(classification_report(y_test, y_pred_no_reg))\n",
        "\n",
        "print(\"\\nClassification Report - With L2 Regularization:\")\n",
        "print(classification_report(y_test, y_pred_with_reg))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Decision Boundary (2D Projection)\n",
        "\n",
        "For visualization, we'll project the data onto the two most important features and visualize the decision boundary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use two most correlated features for visualization\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "\n",
        "# Select two features (e.g., MedInc and AveRooms)\n",
        "feature_idx = [0, 1]  # MedInc and HouseAge\n",
        "X_2d = X_train_scaled[:, feature_idx]\n",
        "\n",
        "# Train a 2D model for visualization\n",
        "model_2d = LogisticRegression(learning_rate=0.1, max_iterations=1000, verbose=False)\n",
        "model_2d.fit(X_2d, y_train)\n",
        "\n",
        "# Create a mesh for decision boundary\n",
        "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
        "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Predict on mesh\n",
        "Z = model_2d.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
        "plt.scatter(X_2d[y_train == 0, 0], X_2d[y_train == 0, 1], \n",
        "           c='blue', marker='o', label='Class 0', alpha=0.6, s=20)\n",
        "plt.scatter(X_2d[y_train == 1, 0], X_2d[y_train == 1, 1], \n",
        "           c='red', marker='s', label='Class 1', alpha=0.6, s=20)\n",
        "plt.xlabel('MedInc (scaled)')\n",
        "plt.ylabel('HouseAge (scaled)')\n",
        "plt.title('Logistic Regression Decision Boundary (2D Projection)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
